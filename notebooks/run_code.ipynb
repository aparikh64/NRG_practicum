{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a27db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    TimeSeriesSplit,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    GroupKFold,\n",
    "    StratifiedGroupKFold,\n",
    ")\n",
    "\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "plt.style.use('classic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6b2732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data for pre-processing\n",
    "df = pd.read_csv('ecoshare_sales_v3.csv', low_memory=False) #opened original file in excel and saved as CSV in excel\n",
    "\n",
    "\n",
    "#engineer features based on order day\n",
    "df['order_day'] = pd.to_datetime(df['order_day'])\n",
    "df['order_day_month'] = df['order_day'].dt.month_name(locale='English')\n",
    "df['order_day_day_of_week'] = df['order_day'].dt.day_name()\n",
    "df['order_day_day_of_month'] = df['order_day'].dt.day\n",
    "df['order_day_quarter'] = pd.PeriodIndex(df.order_day, freq='Q').astype(str).str[-2:]\n",
    "\n",
    "#fill blank values of categorical variables as its own category\n",
    "df['tos_flg'].fillna('blank_value', inplace=True)\n",
    "df['tos_flg'] = df['tos_flg'].astype('category')\n",
    "\n",
    "df['disconotice_flg'].fillna('blank_value', inplace=True)\n",
    "df['disconotice_flg'] = df['disconotice_flg'].astype('category')\n",
    "\n",
    "df['latefee_flg'].fillna('blank_value', inplace=True)\n",
    "df['latefee_flg'] = df['latefee_flg'].astype('category')\n",
    "\n",
    "df['dwelling_type_cd'].fillna('blank_value', inplace=True)\n",
    "df['dwelling_type_cd'] = df['dwelling_type_cd'].astype('category')\n",
    "\n",
    "df['product_type_cd'].fillna('blank_value', inplace=True)\n",
    "df['product_type_cd'] = df['product_type_cd'].astype('category')\n",
    "\n",
    "df['automatic_payment_flg'].fillna('blank_value', inplace=True)\n",
    "df['automatic_payment_flg'] = df['automatic_payment_flg'].astype('category')\n",
    "\n",
    "df['weblog_flg'].fillna('blank_value', inplace=True)\n",
    "df['weblog_flg'] = df['weblog_flg'].astype('category')\n",
    "\n",
    "df['risk_level'].fillna('blank_value', inplace=True)\n",
    "df['risk_level'] = df['risk_level'].astype('category')\n",
    "\n",
    "df['ebill_enroll_flag'].fillna('blank_value', inplace=True)\n",
    "df['ebill_enroll_flag'] = df['ebill_enroll_flag'].astype('category')\n",
    "\n",
    "df['called_flg'].fillna('blank_value', inplace=True)\n",
    "df['called_flg'] = df['called_flg'].astype('category')\n",
    "\n",
    "df['oam_flg'].fillna('blank_value', inplace=True)\n",
    "df['oam_flg'] = df['oam_flg'].astype('category')\n",
    "\n",
    "df['sap_productname'].fillna('blank_value', inplace=True)\n",
    "df['sap_productname'] = df['sap_productname'].astype('category')\n",
    "\n",
    "df['disconnects_flg'].fillna('blank_value', inplace=True)\n",
    "df['disconnects_flg'] = df['disconnects_flg'].astype('category')\n",
    "\n",
    "df['load_profile'].fillna('blank_value', inplace=True)\n",
    "df['load_profile'] = df['load_profile'].astype('category')\n",
    "\n",
    "df['city'].fillna('blank_value', inplace=True)\n",
    "df['city'] = df['city'].astype('category')\n",
    "\n",
    "df['county'].fillna('blank_value', inplace=True)\n",
    "df['county'] = df['county'].astype('category')\n",
    "\n",
    "df['tdsp'].fillna('blank_value', inplace=True)\n",
    "df['tdsp'] = df['tdsp'].astype('category')\n",
    "\n",
    "df['dma'].fillna('blank_value', inplace=True)\n",
    "df['dma'] = df['dma'].astype('category')\n",
    "\n",
    "df['segment'] = 'S' + df['segment'].astype(str)\n",
    "df['segment'].fillna('blank_value', inplace=True)\n",
    "df['segment'] = df['segment'].astype('category')\n",
    "\n",
    "df['order_day_month'].fillna('blank_value', inplace=True)\n",
    "df['order_day_month'] = df['order_day_month'].astype('category')\n",
    "\n",
    "df['order_day_day_of_week'].fillna('blank_value', inplace=True)\n",
    "df['order_day_day_of_week'] = df['order_day_day_of_week'].astype('category')\n",
    "\n",
    "df['order_day_quarter'].fillna('blank_value', inplace=True)\n",
    "df['order_day_quarter'] = df['order_day_quarter'].astype('category')\n",
    "\n",
    "\n",
    "# fill binary blank values with assumed NO (categorical) or 0 (numerical) if blank\n",
    "df['pool'].fillna('N', inplace=True)\n",
    "df['pool'] = df['pool'].astype('category')\n",
    "df['deposit_onhand_amt'] = df['deposit_onhand_amt'].fillna(0)\n",
    "df['deposit_onhand_amt'] = df['deposit_onhand_amt'].astype(np.int64)\n",
    "df['ev_driver'].fillna('N', inplace=True)\n",
    "df['ev_driver'] = df['ev_driver'].astype('category')\n",
    "\n",
    "\n",
    "#fill median values for numerical variables\n",
    "oam_active_login_cnt_median = statistics.median(df['oam_activelogin_cnt'].dropna().to_list())\n",
    "df['oam_activelogin_cnt'] = df['oam_activelogin_cnt'].fillna(oam_active_login_cnt_median)\n",
    "called_numcalls_cnt_median = statistics.median(df['called_numcalls_cnt'].dropna().to_list())\n",
    "df['called_numcalls_cnt'] = df['called_numcalls_cnt'].fillna(called_numcalls_cnt_median)\n",
    "curr_usage_median = statistics.median(df['curr_usage'].dropna().to_list())\n",
    "df['curr_usage'] = df['curr_usage'].fillna(curr_usage_median)\n",
    "df['curr_usage'] = df['curr_usage'].astype(np.int64)\n",
    "numweblog_cnt_median = statistics.median(df['numweblog_cnt'].dropna().to_list())\n",
    "df['numweblog_cnt'] = df['numweblog_cnt'].fillna(numweblog_cnt_median)\n",
    "home_value_median = statistics.median(df['home_value'].dropna().to_list())\n",
    "df['home_value'] = df['home_value'].fillna(home_value_median)\n",
    "df['home_value'] = df['home_value'].astype(np.int64)\n",
    "\n",
    "# unique cases\n",
    "#lots of unique values for term_length so exporting to a CSV and opening in MS Excel\n",
    "#df['term_length'].value_counts(dropna=False).reset_index().rename(columns={\"index\": \"term_length\", \"term_length\": \"count\"}).to_csv('term_length.csv', index=False)\n",
    "#upon review, see 1 null record, 32 records with MM string, and 17 records with C& string\n",
    "#will fill null, and string records with imputed value (median)\n",
    "term_length_impute_list = df['term_length'].dropna().to_list()\n",
    "term_length_impute_list = [i for i in term_length_impute_list if (i != 'MM' and i != 'C&')]\n",
    "term_length_impute_list = list(map(int, term_length_impute_list))\n",
    "term_length_imputed_value = statistics.median(term_length_impute_list)\n",
    "df['term_length'] = df['term_length'].fillna(term_length_imputed_value)\n",
    "df['term_length'] = df['term_length'].replace(['MM', 'C&'], term_length_imputed_value)\n",
    "df['term_length'] = df['term_length'].astype(np.int64)\n",
    "\n",
    "df['order_day_day_of_month'] = df['order_day_day_of_month'].astype(np.int64)\n",
    "\n",
    "df['zipcode'] = 'Z'+df['zipcode'].astype(str).str[:3]\n",
    "df['zipcode'] = df['zipcode'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921f1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complex feature engineering\n",
    "df['year_month'] = pd.DatetimeIndex(df['order_day']).year.astype(str) + '-' + pd.DatetimeIndex(df['order_day']).month.astype(str)\n",
    "\n",
    "accept_rate_by_month = df.groupby('year_month')['accept'].agg(['sum','count']).reset_index()\n",
    "accept_rate_by_month['accept_rate'] = accept_rate_by_month['sum'] / accept_rate_by_month['count']\n",
    "del accept_rate_by_month['sum']\n",
    "#del accept_rate_by_month['count']\n",
    "accept_rate_by_month.columns = ['yr_m', 'call_count','accept_rate']\n",
    "\n",
    "df['order_day_minus_one'] = df['order_day'] - DateOffset(months=1)\n",
    "df['year_month_minus_one'] = pd.DatetimeIndex(df['order_day_minus_one']).year.astype(str) + '-' + pd.DatetimeIndex(df['order_day_minus_one']).month.astype(str)\n",
    "df = df.merge(accept_rate_by_month, how='left', left_on=['year_month_minus_one'], right_on=['yr_m'])\n",
    "df['accept_rate'] = df['accept_rate'].fillna(0)\n",
    "df.rename(columns={\"accept_rate\": \"prior_month_accept_rate\", \"call_count\":\"prior_month_call_count\"}, inplace=True)\n",
    "df['prior_month_call_count'] = df['prior_month_call_count'].fillna(statistics.median(df['prior_month_call_count'].dropna().to_list()))\n",
    "df['prior_month_call_count'] = df['prior_month_call_count'].astype(np.int64)\n",
    "del df['yr_m']\n",
    "\n",
    "\n",
    "# df['order_day_minus_three'] = df['order_day'] - DateOffset(months=3)\n",
    "# df['year_month_minus_three'] = pd.DatetimeIndex(df['order_day_minus_three']).year.astype(str) + '-' + pd.DatetimeIndex(df['order_day_minus_three']).month.astype(str)\n",
    "# df = df.merge(accept_rate_by_month, how='left', left_on=['year_month_minus_three'], right_on=['yr_m'])\n",
    "# del df['yr_m']\n",
    "# df['accept_rate'] = df['accept_rate'].fillna(0)\n",
    "# df.rename(columns={\"accept_rate\": \"prior_three_month_accept_rate\"}, inplace=True)\n",
    "\n",
    "\n",
    "df_test = df.copy()\n",
    "call_attempt = []\n",
    "for index, row in df.iterrows():\n",
    "    c_id = row['customer_id']\n",
    "    or_day = row['order_day']\n",
    "    df_temp = df_test[df_test['customer_id']==c_id]\n",
    "    df_temp = df_temp[df_temp['order_day']<or_day]\n",
    "    call_attempt.append(len(df_temp))\n",
    "df['call_attempt'] = call_attempt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25533b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['tos_flg', 'disconotice_flg', 'oam_activelogin_cnt',\n",
    "       'term_length', 'called_numcalls_cnt', 'latefee_flg', 'dwelling_type_cd',\n",
    "       'curr_usage', 'product_type_cd', 'pool', 'automatic_payment_flg',\n",
    "       'weblog_flg', 'risk_level', 'deposit_onhand_amt', 'ebill_enroll_flag',\n",
    "       'called_flg', 'oam_flg', 'sap_productname', 'numweblog_cnt',\n",
    "       'disconnects_flg', 'load_profile', 'city', 'home_value', 'county',\n",
    "       'tdsp', 'dma', 'ev_driver', 'segment', 'order_day_month',\n",
    "       'order_day_day_of_week', 'order_day_day_of_month', 'order_day_quarter'\n",
    "        ,'zipcode','call_attempt'\n",
    "            #,'prior_month_accept_rate', 'prior_month_call_count'\n",
    "           ]\n",
    "\n",
    "target = 'accept'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89590bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n",
      "[6314, 21314, 36314, 51314, 66314]\n",
      "[6315, 21315, 36315, 51315, 66315]\n",
      "[21314, 36314, 51314, 66314, 81314]\n"
     ]
    }
   ],
   "source": [
    "tss = TimeSeriesSplit(n_splits=5, test_size=15000)\n",
    "train_min = []\n",
    "train_max = []\n",
    "val_min = []\n",
    "val_max = []\n",
    "\n",
    "for train_idx, val_idx in tss.split(df):\n",
    "    train_min.append(train_idx.min())\n",
    "    train_max.append(train_idx.max())\n",
    "    val_min.append(val_idx.min())\n",
    "    val_max.append(val_idx.max())\n",
    "\n",
    "print(train_min)\n",
    "print(train_max)\n",
    "print(val_min)\n",
    "print(val_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba319509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# param_grid = {\n",
    "#     'num_leaves': [50, 100],\n",
    "#     'reg_alpha': [0.1, 0.5],\n",
    "#     'min_data_in_leaf': [30, 50, 100, 300, 400],\n",
    "#     'lambda_l1': [0, 1, 1.5],\n",
    "#     'lambda_l2': [0, 1]\n",
    "#     }\n",
    "    \n",
    "    \n",
    "# clf = lgb.LGBMClassifier(n_estimators = 100)\n",
    "# grid = GridSearchCV(clf, param_grid, cv=tss, scoring=\"roc_auc\")\n",
    "\n",
    "# # Fit the grid search object to the training data\n",
    "# grid.fit(df[features], df[target])\n",
    "# # Print best parameters and score\n",
    "# print(f\"Best parameters: {grid.best_params_}\")\n",
    "# print(f\"Best score: {grid.best_score_}\")\n",
    "\n",
    "#best score returned from this using these parameters:\n",
    "#     clf = lgb.LGBMClassifier(lambda_l1= 0\n",
    "#                              ,lambda_l2= 0\n",
    "#                              ,min_data_in_leaf= 30\n",
    "#                              ,num_leaves= 100\n",
    "#                              ,reg_alpha= 0.1)\n",
    "\n",
    "#pd.DataFrame(grid.cv_results_).to_csv('Grid Search Results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a68031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Info] Number of positive: 539, number of negative: 5776\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1047\n",
      "[LightGBM] [Info] Number of data points in the train set: 6315, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.085352 -> initscore=-2.371751\n",
      "[LightGBM] [Info] Start training from score -2.371751\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Info] Number of positive: 7022, number of negative: 14293\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1440\n",
      "[LightGBM] [Info] Number of data points in the train set: 21315, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.329439 -> initscore=-0.710722\n",
      "[LightGBM] [Info] Start training from score -0.710722\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Info] Number of positive: 7237, number of negative: 29078\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1474\n",
      "[LightGBM] [Info] Number of data points in the train set: 36315, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.199284 -> initscore=-1.390775\n",
      "[LightGBM] [Info] Start training from score -1.390775\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Info] Number of positive: 12839, number of negative: 38476\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1581\n",
      "[LightGBM] [Info] Number of data points in the train set: 51315, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250200 -> initscore=-1.097547\n",
      "[LightGBM] [Info] Start training from score -1.097547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Info] Number of positive: 12998, number of negative: 53317\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1624\n",
      "[LightGBM] [Info] Number of data points in the train set: 66315, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.196004 -> initscore=-1.411460\n",
      "[LightGBM] [Info] Start training from score -1.411460\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n"
     ]
    }
   ],
   "source": [
    "fold = 0\n",
    "preds = []\n",
    "auc_scores = []\n",
    "balanced_accuracy_scores = []\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for train_idx, val_idx in tss.split(df):\n",
    "    tr = df.iloc[train_idx]\n",
    "    te = df.iloc[val_idx]\n",
    "\n",
    "    X_tr = tr[features]\n",
    "    y_tr = tr[target]\n",
    "\n",
    "    X_te = te[features]\n",
    "    y_te = te[target]\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(lambda_l1= 0\n",
    "                             ,lambda_l2= 0\n",
    "                             ,min_data_in_leaf= 30\n",
    "                             ,num_leaves= 100\n",
    "                             ,reg_alpha= 0.1)\n",
    "    \n",
    "    clf.fit(X_tr, y_tr)\n",
    "    \n",
    "    #pred = clf.predict(X_te)\n",
    "\n",
    "    pred_prob = clf.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    auc_score = roc_auc_score(y_te, pred_prob)    \n",
    "    auc_scores.append(auc_score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d3550e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7854438360326522"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "804384ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#potential concerns:\n",
    "#forecasting horizon in test set is not good\n",
    "\n",
    "#ideas that were explored but did not work:\n",
    "#prior month call count\n",
    "#prior month 3 month lag\n",
    "\n",
    "#what i had to do with the feature engineering\n",
    "#my concerns about the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2009c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test data\n",
    "NRG_test_set = pd.read_csv('ecoshare_sales_test_ap.csv', low_memory=False) #the \"_ap\" is because i opened the file in excel and saved as CSV to make the formatting consistent with the training data that was originally provided in xlsx format\n",
    "\n",
    "#engineer features based on order day\n",
    "NRG_test_set['order_day'] = pd.to_datetime(NRG_test_set['order_day'])\n",
    "NRG_test_set['order_day_month'] = NRG_test_set['order_day'].dt.month_name(locale='English')\n",
    "NRG_test_set['order_day_day_of_week'] = NRG_test_set['order_day'].dt.day_name()\n",
    "NRG_test_set['order_day_day_of_month'] = NRG_test_set['order_day'].dt.day\n",
    "NRG_test_set['order_day_quarter'] = pd.PeriodIndex(NRG_test_set.order_day, freq='Q').astype(str).str[-2:]\n",
    "\n",
    "#fill blank values of categorical variables as its own category\n",
    "NRG_test_set['tos_flg'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['tos_flg'] = NRG_test_set['tos_flg'].astype('category')\n",
    "\n",
    "NRG_test_set['disconotice_flg'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['disconotice_flg'] = NRG_test_set['disconotice_flg'].astype('category')\n",
    "\n",
    "NRG_test_set['latefee_flg'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['latefee_flg'] = NRG_test_set['latefee_flg'].astype('category')\n",
    "\n",
    "NRG_test_set['dwelling_type_cd'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['dwelling_type_cd'] = NRG_test_set['dwelling_type_cd'].astype('category')\n",
    "\n",
    "NRG_test_set['product_type_cd'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['product_type_cd'] = NRG_test_set['product_type_cd'].astype('category')\n",
    "\n",
    "NRG_test_set['automatic_payment_flg'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['automatic_payment_flg'] = NRG_test_set['automatic_payment_flg'].astype('category')\n",
    "\n",
    "NRG_test_set['weblog_flg'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['weblog_flg'] = NRG_test_set['weblog_flg'].astype('category')\n",
    "\n",
    "NRG_test_set['risk_level'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['risk_level'] = NRG_test_set['risk_level'].astype('category')\n",
    "\n",
    "NRG_test_set['ebill_enroll_flag'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['ebill_enroll_flag'] = NRG_test_set['ebill_enroll_flag'].astype('category')\n",
    "\n",
    "NRG_test_set['called_flg'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['called_flg'] = NRG_test_set['called_flg'].astype('category')\n",
    "\n",
    "NRG_test_set['oam_flg'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['oam_flg'] = NRG_test_set['oam_flg'].astype('category')\n",
    "\n",
    "NRG_test_set['sap_productname'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['sap_productname'] = NRG_test_set['sap_productname'].astype('category')\n",
    "\n",
    "NRG_test_set['disconnects_flg'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['disconnects_flg'] = NRG_test_set['disconnects_flg'].astype('category')\n",
    "\n",
    "NRG_test_set['load_profile'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['load_profile'] = NRG_test_set['load_profile'].astype('category')\n",
    "\n",
    "NRG_test_set['city'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['city'] = NRG_test_set['city'].astype('category')\n",
    "\n",
    "NRG_test_set['county'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['county'] = NRG_test_set['county'].astype('category')\n",
    "\n",
    "NRG_test_set['tdsp'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['tdsp'] = NRG_test_set['tdsp'].astype('category')\n",
    "\n",
    "NRG_test_set['dma'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['dma'] = NRG_test_set['dma'].astype('category')\n",
    "\n",
    "NRG_test_set['segment'] = 'S' + NRG_test_set['segment'].astype(str)\n",
    "NRG_test_set['segment'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['segment'] = NRG_test_set['segment'].astype('category')\n",
    "\n",
    "NRG_test_set['order_day_month'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['order_day_month'] = NRG_test_set['order_day_month'].astype('category')\n",
    "\n",
    "NRG_test_set['order_day_day_of_week'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['order_day_day_of_week'] = NRG_test_set['order_day_day_of_week'].astype('category')\n",
    "\n",
    "NRG_test_set['order_day_quarter'].fillna('blank_value', inplace=True)\n",
    "NRG_test_set['order_day_quarter'] = NRG_test_set['order_day_quarter'].astype('category')\n",
    "\n",
    "\n",
    "# fill binary blank values with assumed NO (categorical) or 0 (numerical) if blank\n",
    "NRG_test_set['pool'].fillna('N', inplace=True)\n",
    "NRG_test_set['pool'] = NRG_test_set['pool'].astype('category')\n",
    "NRG_test_set['deposit_onhand_amt'] = NRG_test_set['deposit_onhand_amt'].fillna(0)\n",
    "NRG_test_set['deposit_onhand_amt'] = NRG_test_set['deposit_onhand_amt'].astype(np.int64)\n",
    "NRG_test_set['ev_driver'].fillna('N', inplace=True)\n",
    "NRG_test_set['ev_driver'] = NRG_test_set['ev_driver'].astype('category')\n",
    "\n",
    "\n",
    "#fill median values for numerical variables\n",
    "NRG_test_set['oam_activelogin_cnt'] = NRG_test_set['oam_activelogin_cnt'].fillna(oam_active_login_cnt_median)\n",
    "NRG_test_set['called_numcalls_cnt'] = NRG_test_set['called_numcalls_cnt'].fillna(called_numcalls_cnt_median)\n",
    "NRG_test_set['curr_usage'] = NRG_test_set['curr_usage'].fillna(curr_usage_median)\n",
    "NRG_test_set['curr_usage'] = NRG_test_set['curr_usage'].astype(np.int64)\n",
    "NRG_test_set['numweblog_cnt'] = NRG_test_set['numweblog_cnt'].fillna(numweblog_cnt_median)\n",
    "NRG_test_set['home_value'] = NRG_test_set['home_value'].fillna(home_value_median)\n",
    "NRG_test_set['home_value'] = NRG_test_set['home_value'].astype(np.int64)\n",
    "\n",
    "\n",
    "# unique cases\n",
    "NRG_test_set['term_length'] = NRG_test_set['term_length'].fillna(term_length_imputed_value)\n",
    "NRG_test_set['term_length'] = NRG_test_set['term_length'].replace(['MM', 'C&'], term_length_imputed_value)\n",
    "NRG_test_set['term_length'] = NRG_test_set['term_length'].astype(np.int64)\n",
    "\n",
    "NRG_test_set['order_day_day_of_month'] = NRG_test_set['order_day_day_of_month'].astype(np.int64)\n",
    "\n",
    "NRG_test_set['zipcode'] = 'Z'+NRG_test_set['zipcode'].astype(str).str[:3]\n",
    "NRG_test_set['zipcode'] = NRG_test_set['zipcode'].astype('category')\n",
    "\n",
    "#complex feature engineering\n",
    "call_attempt = []\n",
    "for index, row in NRG_test_set.iterrows():\n",
    "    c_id = row['customer_id']\n",
    "    or_day = row['order_day']\n",
    "    base_number = len(df[df['customer_id']==c_id])\n",
    "    temp = NRG_test_set[NRG_test_set['customer_id']==c_id]\n",
    "    temp = temp[temp['order_day']<or_day]\n",
    "    call_attempt.append(len(df_temp)+base_number)\n",
    "NRG_test_set['call_attempt'] = call_attempt\n",
    "\n",
    "del NRG_test_set['customer_id']\n",
    "del NRG_test_set['order_day']\n",
    "del NRG_test_set['meter_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71664b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Info] Number of positive: 13724, number of negative: 67591\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1683\n",
      "[LightGBM] [Info] Number of data points in the train set: 81315, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.168776 -> initscore=-1.594329\n",
      "[LightGBM] [Info] Start training from score -1.594329\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n"
     ]
    }
   ],
   "source": [
    "# fit on whole training set now\n",
    "clf = lgb.LGBMClassifier(lambda_l1= 0\n",
    "                         ,lambda_l2= 0\n",
    "                         ,min_data_in_leaf= 30\n",
    "                         ,num_leaves= 100\n",
    "                         ,reg_alpha= 0.1)\n",
    "\n",
    "clf.fit(df[features], df[target])\n",
    "\n",
    "#predict on test set\n",
    "NRG_final_preds = clf.predict_proba(NRG_test_set)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee709786",
   "metadata": {},
   "outputs": [],
   "source": [
    "NRG_final_preds_df = pd.DataFrame(NRG_final_preds, columns=[\"probability\"])\n",
    "NRG_final_preds_df.to_pickle('aditya_parikh_predictions.pkl')\n",
    "NRG_final_preds_df.to_csv('aditya_parikh_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
